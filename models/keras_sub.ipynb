{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fit word2vec on full/test questions\n",
    "# fit tokenizer on full/test questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 760 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "import gensim, logging\n",
    "import json\n",
    "\n",
    "import os.path\n",
    "\n",
    "MAX_NUM_WORDS = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def submit(y_pred, test, filename):\n",
    "    sub = pd.DataFrame()\n",
    "    sub = pd.DataFrame()\n",
    "    sub['test_id'] = test['test_id']\n",
    "    sub['is_duplicate'] = y_test\n",
    "    sub.to_csv(filename, index=False)\n",
    "\n",
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((  loader['data'], loader['indices'], loader['indptr']),\n",
    "                         shape = loader['shape'])\n",
    "\n",
    "def correct_dataset(dataset):\n",
    "    dataset.loc[(dataset['question1'] == dataset['question2']), 'is_duplicate'] = 1\n",
    "    return dataset\n",
    "\n",
    "def process_dataset(dataset, correct_dataset=False):\n",
    "    dataset['question1'].fillna(' ', inplace=True)\n",
    "    dataset['question2'].fillna(' ', inplace=True)\n",
    "    \n",
    "    #delete punctuation\n",
    "    dataset['question1'] = dataset['question1'].str.replace('[^\\w\\s]','')\n",
    "    dataset['question2'] = dataset['question2'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "    #lower questions\n",
    "    dataset['question1'] = dataset['question1'].str.lower()\n",
    "    dataset['question2'] = dataset['question2'].str.lower()\n",
    "\n",
    "    #union questions\n",
    "    dataset['union'] = pd.Series(dataset['question1']).str.cat(dataset['question2'], sep=' ')\n",
    "\n",
    "    if correct_dataset:\n",
    "        return correct_dataset(dataset)\n",
    "    else:\n",
    "        return dataset\n",
    "\n",
    "def split_and_rem_stop_words(line):\n",
    "    cachedStopWords = stopwords.words(\"english\")\n",
    "    return [word for word in line.split() if word not in cachedStopWords]\n",
    "\n",
    "def create_word_to_vec(sentences, embedding_path, verbose=0, save=1, **params_for_w2v):\n",
    "    if verbose:\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    model = gensim.models.Word2Vec(sentences, **params_for_w2v)\n",
    "    \n",
    "    if save:\n",
    "        model.save(embedding_path)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "def create_embeddings(sentences, embeddings_path='embeddings/embedding.npz',\n",
    "                      verbose=0, **params):\n",
    "    \"\"\"\n",
    "    Generate embeddings from a batch of text\n",
    "    :param embeddings_path: where to save the embeddings\n",
    "    :param vocab_path: where to save the word-index map\n",
    "    \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    model = gensim.models.Word2Vec(sentences, **params)\n",
    "    weights = model.wv.syn0\n",
    "    np.save(open(embeddings_path, 'wb'), weights)\n",
    "\n",
    "\n",
    "def load_vocab(vocab_path):\n",
    "    \"\"\"\n",
    "    Load word -> index and index -> word mappings\n",
    "    :param vocab_path: where the word-index map is saved\n",
    "    :return: word2idx, idx2word\n",
    "    \"\"\"\n",
    "\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    word2idx = data\n",
    "    idx2word = dict([(v, k) for k, v in data.items()])\n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "def get_word2vec_embedding_layer(embeddings_path):\n",
    "    \"\"\"\n",
    "    Generate an embedding layer word2vec embeddings\n",
    "    :param embeddings_path: where the embeddings are saved (as a numpy file)\n",
    "    :return: the generated embedding layer\n",
    "    \"\"\"\n",
    "\n",
    "    weights = np.load(open(embeddings_path, 'rb'))\n",
    "    layer = Embedding(input_dim=weights.shape[0], output_dim=weights.shape[1], weights=[weights],\n",
    "                     trainable=False)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Load train\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isfile('dataframes/train.h5'):\n",
    "    train = pd.read_pickle('dataframes/train.h5')\n",
    "else:\n",
    "    train = pd.read_csv('../datasets/train.csv')\n",
    "    train = process_dataset(train)\n",
    "    train['union_splitted'] = train['union'].apply(lambda sentence: split_and_rem_stop_words(sentence))\n",
    "    train.to_pickle('dataframes/train.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load test\n",
    "\n",
    "if all([os.path.isfile('dataframes/test_0.h5'), os.path.isfile('dataframes/test_1.h5'),\n",
    "        os.path.isfile('dataframes/test_2.h5'), os.path.isfile('dataframes/test_3.h5')]):\n",
    "    \n",
    "    test = pd.read_csv('../datasets/test.csv')\n",
    "    test = process_dataset(test)\n",
    "    \n",
    "#     test_0 = pd.read_pickle('dataframes/test_0.h5')\n",
    "#     test_1 = pd.read_pickle('dataframes/test_1.h5')\n",
    "#     test_2 = pd.read_pickle('dataframes/test_2.h5')\n",
    "#     test_3 = pd.read_pickle('dataframes/test_3.h5')\n",
    "\n",
    "#     test_0.columns = ['union_splitted']\n",
    "#     test_1.columns = ['union_splitted']\n",
    "#     test_2.columns = ['union_splitted']\n",
    "#     test_3.columns = ['union_splitted']\n",
    "\n",
    "#     test_full_splitted = test_0.append(\n",
    "#                          test_1.append(\n",
    "#                          test_2.append(\n",
    "#                          test_3)))\n",
    "\n",
    "#     test['union_splitted'] = test_full_splitted['union_splitted'].values\n",
    "else:\n",
    "    print 'Not enough files for test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 108180 unique tokens.\n",
      "('Shape of data tensor:', (2345796, 125))\n"
     ]
    }
   ],
   "source": [
    "#Tokenize test\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NUM_WORDS, split=' ')\n",
    "tokenizer.fit_on_texts(train['union'])\n",
    "sequences = tokenizer.texts_to_sequences(test['union'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_test = pad_sequences(sequences, maxlen=MAX_NUM_WORDS)\n",
    "\n",
    "print('Shape of data tensor:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4caf434906fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/loopdigga/Documents/ml/ml_env/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/loopdigga/Documents/ml/ml_env/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1272\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/home/loopdigga/Documents/ml/ml_env/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/loopdigga/Documents/ml/ml_env/local/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/loopdigga/Documents/ml/ml_env/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#Load model\n",
    "\n",
    "model = load_model('keras_models/my_model_6_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict\n",
    "\n",
    "y_preds = model.predict(X_test, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_num_words = train['union_splitted'].map(len).max()\n",
    "len_x = len(train['union_splitted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 108180 unique tokens.\n",
      "('Shape of data tensor:', (404290, 125))\n"
     ]
    }
   ],
   "source": [
    "#Tokenize train\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NUM_WORDS, split=' ')\n",
    "tokenizer.fit_on_texts(train['union'])\n",
    "sequences = tokenizer.texts_to_sequences(train['union'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_train = pad_sequences(sequences, maxlen=MAX_NUM_WORDS)\n",
    "y_train = train.is_duplicate.tolist()\n",
    "\n",
    "print('Shape of data tensor:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-12 15:22:55,259 : INFO : collecting all words and their counts\n",
      "2017-04-12 15:22:55,262 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-12 15:22:55,292 : INFO : PROGRESS: at sentence #10000, processed 110842 words, keeping 15889 word types\n",
      "2017-04-12 15:22:55,322 : INFO : PROGRESS: at sentence #20000, processed 221497 words, keeping 23104 word types\n",
      "2017-04-12 15:22:55,349 : INFO : PROGRESS: at sentence #30000, processed 332393 words, keeping 28724 word types\n",
      "2017-04-12 15:22:55,376 : INFO : PROGRESS: at sentence #40000, processed 442629 words, keeping 33290 word types\n",
      "2017-04-12 15:22:55,401 : INFO : PROGRESS: at sentence #50000, processed 553973 words, keeping 37495 word types\n",
      "2017-04-12 15:22:55,427 : INFO : PROGRESS: at sentence #60000, processed 664660 words, keeping 41167 word types\n",
      "2017-04-12 15:22:55,455 : INFO : PROGRESS: at sentence #70000, processed 775454 words, keeping 44525 word types\n",
      "2017-04-12 15:22:55,482 : INFO : PROGRESS: at sentence #80000, processed 886679 words, keeping 47538 word types\n",
      "2017-04-12 15:22:55,510 : INFO : PROGRESS: at sentence #90000, processed 997109 words, keeping 50378 word types\n",
      "2017-04-12 15:22:55,537 : INFO : PROGRESS: at sentence #100000, processed 1107801 words, keeping 53304 word types\n",
      "2017-04-12 15:22:55,563 : INFO : PROGRESS: at sentence #110000, processed 1217243 words, keeping 55932 word types\n",
      "2017-04-12 15:22:55,590 : INFO : PROGRESS: at sentence #120000, processed 1327824 words, keeping 58361 word types\n",
      "2017-04-12 15:22:55,614 : INFO : PROGRESS: at sentence #130000, processed 1439135 words, keeping 60859 word types\n",
      "2017-04-12 15:22:55,639 : INFO : PROGRESS: at sentence #140000, processed 1550433 words, keeping 63217 word types\n",
      "2017-04-12 15:22:55,664 : INFO : PROGRESS: at sentence #150000, processed 1661890 words, keeping 65626 word types\n",
      "2017-04-12 15:22:55,691 : INFO : PROGRESS: at sentence #160000, processed 1773442 words, keeping 67889 word types\n",
      "2017-04-12 15:22:55,719 : INFO : PROGRESS: at sentence #170000, processed 1884607 words, keeping 69904 word types\n",
      "2017-04-12 15:22:55,747 : INFO : PROGRESS: at sentence #180000, processed 1996244 words, keeping 71998 word types\n",
      "2017-04-12 15:22:55,772 : INFO : PROGRESS: at sentence #190000, processed 2107414 words, keeping 73910 word types\n",
      "2017-04-12 15:22:55,803 : INFO : PROGRESS: at sentence #200000, processed 2217690 words, keeping 75809 word types\n",
      "2017-04-12 15:22:55,831 : INFO : PROGRESS: at sentence #210000, processed 2328075 words, keeping 77682 word types\n",
      "2017-04-12 15:22:55,856 : INFO : PROGRESS: at sentence #220000, processed 2438747 words, keeping 79489 word types\n",
      "2017-04-12 15:22:55,884 : INFO : PROGRESS: at sentence #230000, processed 2549833 words, keeping 81295 word types\n",
      "2017-04-12 15:22:55,907 : INFO : PROGRESS: at sentence #240000, processed 2660238 words, keeping 82994 word types\n",
      "2017-04-12 15:22:55,932 : INFO : PROGRESS: at sentence #250000, processed 2771487 words, keeping 84820 word types\n",
      "2017-04-12 15:22:55,961 : INFO : PROGRESS: at sentence #260000, processed 2883236 words, keeping 86574 word types\n",
      "2017-04-12 15:22:55,990 : INFO : PROGRESS: at sentence #270000, processed 2995182 words, keeping 88277 word types\n",
      "2017-04-12 15:22:56,016 : INFO : PROGRESS: at sentence #280000, processed 3106000 words, keeping 89921 word types\n",
      "2017-04-12 15:22:56,045 : INFO : PROGRESS: at sentence #290000, processed 3216907 words, keeping 91541 word types\n",
      "2017-04-12 15:22:56,069 : INFO : PROGRESS: at sentence #300000, processed 3327044 words, keeping 93172 word types\n",
      "2017-04-12 15:22:56,098 : INFO : PROGRESS: at sentence #310000, processed 3437812 words, keeping 94709 word types\n",
      "2017-04-12 15:22:56,128 : INFO : PROGRESS: at sentence #320000, processed 3548152 words, keeping 96171 word types\n",
      "2017-04-12 15:22:56,155 : INFO : PROGRESS: at sentence #330000, processed 3659521 words, keeping 97707 word types\n",
      "2017-04-12 15:22:56,184 : INFO : PROGRESS: at sentence #340000, processed 3769913 words, keeping 99101 word types\n",
      "2017-04-12 15:22:56,213 : INFO : PROGRESS: at sentence #350000, processed 3880766 words, keeping 100536 word types\n",
      "2017-04-12 15:22:56,243 : INFO : PROGRESS: at sentence #360000, processed 3991804 words, keeping 101954 word types\n",
      "2017-04-12 15:22:56,274 : INFO : PROGRESS: at sentence #370000, processed 4102433 words, keeping 103335 word types\n",
      "2017-04-12 15:22:56,306 : INFO : PROGRESS: at sentence #380000, processed 4213923 words, keeping 104808 word types\n",
      "2017-04-12 15:22:56,337 : INFO : PROGRESS: at sentence #390000, processed 4326079 words, keeping 106244 word types\n",
      "2017-04-12 15:22:56,366 : INFO : PROGRESS: at sentence #400000, processed 4438291 words, keeping 107566 word types\n",
      "2017-04-12 15:22:56,380 : INFO : collected 108152 word types from a corpus of 4486323 raw words and 404290 sentences\n",
      "2017-04-12 15:22:56,380 : INFO : Loading a fresh vocabulary\n",
      "2017-04-12 15:22:56,495 : INFO : min_count=5 retains 31660 unique words (29% of original 108152, drops 76492)\n",
      "2017-04-12 15:22:56,496 : INFO : min_count=5 leaves 4366911 word corpus (97% of original 4486323, drops 119412)\n",
      "2017-04-12 15:22:56,565 : INFO : deleting the raw counts dictionary of 108152 items\n",
      "2017-04-12 15:22:56,570 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2017-04-12 15:22:56,571 : INFO : downsampling leaves estimated 4207849 word corpus (96.4% of prior 4366911)\n",
      "2017-04-12 15:22:56,571 : INFO : estimated required memory for 31660 words and 100 dimensions: 41158000 bytes\n",
      "2017-04-12 15:22:56,674 : INFO : resetting layer weights\n",
      "2017-04-12 15:22:56,936 : INFO : training model with 3 workers on 31660 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-04-12 15:22:57,947 : INFO : PROGRESS: at 6.30% examples, 1313877 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-12 15:22:58,950 : INFO : PROGRESS: at 13.11% examples, 1371247 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-12 15:22:59,954 : INFO : PROGRESS: at 19.87% examples, 1386943 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-12 15:23:00,955 : INFO : PROGRESS: at 26.48% examples, 1386573 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-12 15:23:01,965 : INFO : PROGRESS: at 31.55% examples, 1320490 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-12 15:23:02,970 : INFO : PROGRESS: at 37.79% examples, 1318146 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-12 15:23:03,972 : INFO : PROGRESS: at 43.98% examples, 1315573 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-12 15:23:04,974 : INFO : PROGRESS: at 51.69% examples, 1353389 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-12 15:23:05,974 : INFO : PROGRESS: at 59.35% examples, 1381949 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-12 15:23:06,976 : INFO : PROGRESS: at 67.06% examples, 1405561 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-12 15:23:07,980 : INFO : PROGRESS: at 74.86% examples, 1426234 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-12 15:23:08,985 : INFO : PROGRESS: at 82.65% examples, 1443462 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-12 15:23:09,986 : INFO : PROGRESS: at 90.41% examples, 1457797 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-12 15:23:10,987 : INFO : PROGRESS: at 98.12% examples, 1469311 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-12 15:23:11,221 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-12 15:23:11,226 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-12 15:23:11,228 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-12 15:23:11,229 : INFO : training on 22431615 raw words (21039307 effective words) took 14.3s, 1472407 effective words/s\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('../embeddings/embedding.npz'):\n",
    "    create_embeddings(sentences=train['union_splitted'], embeddings_path='../embeddings/embedding.npz', verbose=1)\n",
    "\n",
    "weights = np.load(open('../embeddings/embedding.npz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../embeddings/embedding.npz'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3c22c3a7e831>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../embeddings/embedding.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m embedding_layer = Embedding(input_dim=weights.shape[0], output_dim=100, weights=[weights], \n\u001b[1;32m      4\u001b[0m                             input_length=max_num_words, trainable=False)\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# # embedding_layer = get_word2vec_embedding_layer('embeddings/embedding.npz')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../embeddings/embedding.npz'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "embedding_layer = Embedding(input_dim=weights.shape[0], output_dim=100, weights=[weights], \n",
    "                            input_length=max_num_words, trainable=False)\n",
    "# # embedding_layer = get_word2vec_embedding_layer('embeddings/embedding.npz')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(16, 2, activation='relu'))\n",
    "# model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Conv1D(32, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Conv1D(64, 4, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, nb_epoch=1, \n",
    "          validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}